import requests
import os
from collections import Counter
import re
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class NewsAnalyzer:
    def __init__(self):
        self.api_key = os.getenv('NEWS_API_KEY')
        self.base_url = 'https://newsapi.org/v2'
        
        if not self.api_key:
            raise ValueError("í™˜ê²½ ë³€ìˆ˜ NEWS_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def search_news(self, query, language='ko', page_size=100):
        """ë‰´ìŠ¤ ê²€ìƒ‰"""
        url = f'{self.base_url}/everything'
        params = {
            'q': query,
            'language': language,
            'pageSize': page_size,
            'apiKey': self.api_key
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f'ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}')
            return None
    
    def extract_keywords(self, text, top_n=10):
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í•œê¸€, ì˜ë¬¸ë§Œ ì¶”ì¶œ (ìµœì†Œ 2ê¸€ì)
        words = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', text)
        
        # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        stopwords = {'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ìˆë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤'}
        words = [w for w in words if w not in stopwords]
        
        # ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(words)
        return word_counts.most_common(top_n)
    
    def analyze_news_trends(self, query):
        """ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„"""
        data = self.search_news(query)
        
        if not data or data['totalResults'] == 0:
            print('ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            return
        
        articles = data['articles']
        
        # ëª¨ë“  ê¸°ì‚¬ ì œëª©ê³¼ ì„¤ëª… í•©ì¹˜ê¸°
        all_text = ' '.join([
            (article.get('title', '') + ' ' + article.get('description', ''))
            for article in articles
        ])
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(all_text, top_n=15)
        
        print(f"\nğŸ“° '{query}' ê´€ë ¨ ë‰´ìŠ¤ ë¶„ì„")
        print(f"{'='*40}")
        print(f"ì´ ê¸°ì‚¬ ìˆ˜: {data['totalResults']}")
        print(f"\nì£¼ìš” í‚¤ì›Œë“œ:")
        
        for word, count in keywords:
            print(f"  {word}: {count}íšŒ")
        
        print(f"\nìµœì‹  ë‰´ìŠ¤ 3ê±´:")
        for i, article in enumerate(articles[:3], 1):
            print(f"\n{i}. {article['title']}")
            print(f"   ì¶œì²˜: {article['source']['name']}")
            print(f"   ë§í¬: {article['url']}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    try:
        analyzer = NewsAnalyzer()
        analyzer.analyze_news_trends('ì¸ê³µì§€ëŠ¥')
    except ValueError as e:
        print(f"ì˜¤ë¥˜: {e}")
        print("News API í‚¤ë¥¼ ë°œê¸‰ë°›ìœ¼ì„¸ìš”: https://newsapi.org")